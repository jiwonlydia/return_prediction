{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0613_run",
      "provenance": [],
      "collapsed_sections": [
        "fQGC2EBjOcpZ",
        "afxm5qvLg8Y7"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMuACkY+O4Y3d7GjlWMjLLt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiwonlydia/return_prediction/blob/main/0613_run.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "공식 깃허브: https://github.com/LouisChen1992/Deep_Learning_Asset_Pricing"
      ],
      "metadata": {
        "id": "cPjMM4PE7kaN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Colab Setup"
      ],
      "metadata": {
        "id": "fQGC2EBjOcpZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MFAaRefOhe2",
        "outputId": "c0bd868e-5643-4c42-960c-ec01e8ab1adf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path = '/content/drive/MyDrive/research/return_prediction/dlap'\n",
        "os.chdir(path)"
      ],
      "metadata": {
        "id": "e4EAqmKPOkww"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow==1.14.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_B3wNT--ypLg",
        "outputId": "82fca104-59ed-41f6-df65-0e219affdebe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==1.14.0\n",
            "  Downloading tensorflow-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (109.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 109.3 MB 43 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.21.6)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (3.17.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.37.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.2)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "  Downloading tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488 kB)\n",
            "\u001b[K     |████████████████████████████████| 488 kB 10.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.46.3)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.5.3)\n",
            "Collecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.6 MB/s \n",
            "\u001b[?25hCollecting tensorboard<1.15.0,>=1.14.0\n",
            "  Downloading tensorboard-1.14.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 27.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.14.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.3.7)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (4.2.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.14.0) (1.5.2)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220527125636\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220527125636:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220527125636\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.14.0 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-applications-1.0.8 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "afxm5qvLg8Y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "from src.data import data_layer\n",
        "from src.utils import deco_print"
      ],
      "metadata": {
        "id": "h4EaaThTOqqC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ceff77b-7f46-4da2-f52f-6a655c2dc24b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.14.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Delete all flags before declare #####\n",
        "\n",
        "def del_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()    \n",
        "    keys_list = [keys for keys in flags_dict]    \n",
        "    for keys in keys_list:\n",
        "        FLAGS.__delattr__(keys)"
      ],
      "metadata": {
        "id": "d4Bf5D6SR6RP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- tensorflow의 flag란?\n",
        "- https://daeson.tistory.com/256"
      ],
      "metadata": {
        "id": "pXv6SbUwVMIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# del_all_flags(tf.flags.FLAGS)\n",
        "\n",
        "tf.flags.DEFINE_string(\"f\", \"\", \"kernel\") # 주피터 노트북에서 돌릴땐 이 코드 추가해야 돌아감\n",
        "\n",
        "tf.flags.DEFINE_string('config', path+'/config/config.json', 'Path to the file with configurations')\n",
        "tf.flags.DEFINE_string('logdir', '', 'Path to save logs and checkpoints')\n",
        "\n",
        "tf.flags.DEFINE_integer('saveBestFreq', -1, 'Frequency to save best model')\n",
        "tf.flags.DEFINE_boolean('printOnConsole', True, 'Print on console or not')\n",
        "tf.flags.DEFINE_boolean('saveLog', True, 'Save log or not')\n",
        "tf.flags.DEFINE_integer('printFreq', 128, 'Frequency to print on console')\n",
        "tf.flags.DEFINE_integer('ignoreEpoch', 64, 'Ignore first several epochs')\n",
        "\n",
        "FLAGS = tf.flags.FLAGS\n",
        "print(FLAGS.config)\n",
        "print(FLAGS.logdir)\n",
        "print(FLAGS.saveBestFreq)\n",
        "print(FLAGS.printOnConsole)\n",
        "print(FLAGS.saveLog)\n",
        "print(FLAGS.printFreq)\n",
        "print(FLAGS.ignoreEpoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jzx2JmBrO4G4",
        "outputId": "0e6f7d47-7c5f-4152-eeba-bdc762636db4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/research/return_prediction/dlap/config/config.json\n",
            "\n",
            "-1\n",
            "True\n",
            "True\n",
            "128\n",
            "64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(FLAGS.config, 'r') as file:\n",
        "    config = json.load(file)\n",
        "    if not 'macro_idx' in config:\n",
        "        config['macro_idx'] = None\n",
        "\n",
        "deco_print('Read the following in config: ')\n",
        "print(json.dumps(config, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97zcXLnWW-rw",
        "outputId": "36845045-e033-41b7-fdfa-f0fe378e2a99"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">==================> Read the following in config: \n",
            "{\n",
            "    \"learning_rate\": 0.001,\n",
            "    \"num_layers_moment\": 0,\n",
            "    \"num_units_rnn\": [\n",
            "        4\n",
            "    ],\n",
            "    \"macro_feature_file_test\": \"datasets/macro/macro_test.npz\",\n",
            "    \"optimizer\": \"Adam\",\n",
            "    \"macro_feature_file\": \"datasets/macro/macro_train.npz\",\n",
            "    \"use_rnn\": true,\n",
            "    \"weighted_loss\": true,\n",
            "    \"hidden_dim\": [\n",
            "        64,\n",
            "        64\n",
            "    ],\n",
            "    \"cell_type_rnn_moment\": \"lstm\",\n",
            "    \"cell_type_rnn\": \"lstm\",\n",
            "    \"macro_feature_file_valid\": \"datasets/macro/macro_valid.npz\",\n",
            "    \"num_epochs_moment\": 64,\n",
            "    \"tSize_test\": 300,\n",
            "    \"tSize\": 240,\n",
            "    \"individual_feature_dim\": 46,\n",
            "    \"tSize_valid\": 60,\n",
            "    \"num_condition_moment\": 8,\n",
            "    \"loss_factor\": 1.0,\n",
            "    \"num_layers_rnn_moment\": 1,\n",
            "    \"individual_feature_file_valid\": \"datasets/char/Char_valid.npz\",\n",
            "    \"num_epochs\": 1024,\n",
            "    \"individual_feature_file\": \"datasets/char/Char_train.npz\",\n",
            "    \"num_units_rnn_moment\": [\n",
            "        32\n",
            "    ],\n",
            "    \"num_epochs_unc\": 256,\n",
            "    \"dropout\": 0.95,\n",
            "    \"macro_feature_dim\": 178,\n",
            "    \"num_layers_rnn\": 1,\n",
            "    \"sub_epoch\": 4,\n",
            "    \"individual_feature_file_test\": \"datasets/char/Char_test.npz\",\n",
            "    \"hidden_dim_moment\": [],\n",
            "    \"num_layers\": 2,\n",
            "    \"macro_idx\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCyb1U70hLfW",
        "outputId": "509314fa-d895-4167-e701-45de72b02015"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cell_type_rnn': 'lstm',\n",
              " 'cell_type_rnn_moment': 'lstm',\n",
              " 'dropout': 0.95,\n",
              " 'hidden_dim': [64, 64],\n",
              " 'hidden_dim_moment': [],\n",
              " 'individual_feature_dim': 46,\n",
              " 'individual_feature_file': 'datasets/char/Char_train.npz',\n",
              " 'individual_feature_file_test': 'datasets/char/Char_test.npz',\n",
              " 'individual_feature_file_valid': 'datasets/char/Char_valid.npz',\n",
              " 'learning_rate': 0.001,\n",
              " 'loss_factor': 1.0,\n",
              " 'macro_feature_dim': 178,\n",
              " 'macro_feature_file': 'datasets/macro/macro_train.npz',\n",
              " 'macro_feature_file_test': 'datasets/macro/macro_test.npz',\n",
              " 'macro_feature_file_valid': 'datasets/macro/macro_valid.npz',\n",
              " 'macro_idx': None,\n",
              " 'num_condition_moment': 8,\n",
              " 'num_epochs': 1024,\n",
              " 'num_epochs_moment': 64,\n",
              " 'num_epochs_unc': 256,\n",
              " 'num_layers': 2,\n",
              " 'num_layers_moment': 0,\n",
              " 'num_layers_rnn': 1,\n",
              " 'num_layers_rnn_moment': 1,\n",
              " 'num_units_rnn': [4],\n",
              " 'num_units_rnn_moment': [32],\n",
              " 'optimizer': 'Adam',\n",
              " 'sub_epoch': 4,\n",
              " 'tSize': 240,\n",
              " 'tSize_test': 300,\n",
              " 'tSize_valid': 60,\n",
              " 'use_rnn': True,\n",
              " 'weighted_loss': True}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "b6qb9xiyhAsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dl = data_layer.DataInRamInputLayer(\n",
        "\t\tconfig['individual_feature_file'],\n",
        "\t\tpathMacroFeature=config['macro_feature_file'],\n",
        "\t\tmacroIdx=config['macro_idx']) # 0 ~ 177\n",
        "\t\n",
        "# normalize 하기 위해 macro 변수의 mean, std 계산\n",
        "meanMacroFeature, stdMacroFeature = dl.getMacroFeatureMeanStd()\n",
        "\n",
        "dl_valid = data_layer.DataInRamInputLayer(\n",
        "    config['individual_feature_file_valid'],\n",
        "    pathMacroFeature=config['macro_feature_file_valid'],\n",
        "    macroIdx=config['macro_idx'], \n",
        "    meanMacroFeature=meanMacroFeature, # train에서 계산한 값으로 mean 설정\n",
        "    stdMacroFeature=stdMacroFeature) # train에서 계산한 값으로 std 설정\n",
        "\n",
        "dl_test = data_layer.DataInRamInputLayer(\n",
        "    config['individual_feature_file_test'],\n",
        "    pathMacroFeature=config['macro_feature_file_test'],\n",
        "    macroIdx=config['macro_idx'], \n",
        "    meanMacroFeature=meanMacroFeature, # train에서 계산한 값으로 mean 설정\n",
        "    stdMacroFeature=stdMacroFeature) # train에서 계산한 값으로 std 설정\n",
        "\n",
        "if config['weighted_loss']: # True\n",
        "    # 회사 3686개에 대해 return값이 존재하는 날짜의 개수를 합함 (axis=0) => 회사별 weight\n",
        "    loss_weight = dl.getDateCountList() # shape (3686,)\n",
        "    loss_weight_valid = dl_valid.getDateCountList()\n",
        "    loss_weight_test = dl_test.getDateCountList()\n",
        "else:\n",
        "    loss_weight = None\n",
        "    loss_weight_valid = None\n",
        "    loss_weight_test = None\n",
        "deco_print('Data layer created')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCcl-sCvzpyW",
        "outputId": "792edfc5-5da1-4638-bf20-f70dd3005ec6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">==================> Data layer created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_params = config"
      ],
      "metadata": {
        "id": "PKNKsh30iG3P"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM\n",
        "---\n",
        "- https://wdprogrammer.tistory.com/34"
      ],
      "metadata": {
        "id": "R_w7_TVcoakc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.rnn_cell import BasicRNNCell\n",
        "from tensorflow.python.ops.rnn_cell import GRUCell\n",
        "from tensorflow.python.ops.rnn_cell import LSTMCell\n",
        "from tensorflow.python.ops.rnn_cell import DropoutWrapper\n",
        "from tensorflow.python.ops.rnn_cell import MultiRNNCell\n",
        "\n",
        "def create_rnn_cell(cell_type, num_units, num_layers=1, dp_input_keep_prob=1.0, dp_output_keep_prob=1.0):\n",
        "\tdef single_cell(num_units):\n",
        "\t\tif cell_type == 'rnn':\n",
        "\t\t\tcell_class = BasicRNNCell\n",
        "\t\telif cell_type == 'gru':\n",
        "\t\t\tcell_class = GRUCell\n",
        "\t\telif cell_type == 'lstm':\n",
        "\t\t\tcell_class = LSTMCell\n",
        "\t\telse:\n",
        "\t\t\traise ValueError('Cell Type Not Supported! ')\n",
        "\n",
        "\t\tif dp_input_keep_prob != 1.0 or dp_output_keep_prob != 1.0:\n",
        "\t\t\treturn DropoutWrapper(cell_class(num_units=num_units),\n",
        "\t\t\t\tinput_keep_prob=dp_input_keep_prob,\n",
        "\t\t\t\toutput_keep_prob=dp_output_keep_prob)\n",
        "\t\telse:\n",
        "\t\t\treturn cell_class(num_units=num_units)\n",
        "\n",
        "\tassert(len(num_units) == num_layers)\n",
        "\tif num_layers > 1:\n",
        "\t\treturn MultiRNNCell([single_cell(num_units[i]) for i in range(num_layers)])\n",
        "\telse:\n",
        "\t\treturn single_cell(num_units[0])"
      ],
      "metadata": {
        "id": "DxvNXLWNjt_8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initial_state_size(cell_type, num_units):\n",
        "\tstate_size = sum(num_units)\n",
        "\t\treturn state_size * 2"
      ],
      "metadata": {
        "id": "4h4olN3SCmag"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.layers.core import Dense\n",
        "from tensorflow.python.ops.rnn_cell_impl import LSTMStateTuple\n",
        "\n",
        "class Model():\n",
        "    def __init__(self, model_params, tSize, global_step=None):\n",
        "        # super(Model, self).__init__(model_params, global_step)\n",
        "        self.model_params = model_params\n",
        "        self._global_step = global_step\n",
        "\n",
        "        # ---- placeholders ---- #\n",
        "        self._tSize = tSize\n",
        "        self._macro_feature_dim = self.model_params['macro_feature_dim']\n",
        "        self._individual_feature_dim = self.model_params['individual_feature_dim']\n",
        "        self._I_macro_placeholder = tf.placeholder(dtype=tf.float32, shape=[self._tSize, self._macro_feature_dim], name='macroFeaturePlaceholder')\n",
        "        self._I_placeholder = tf.placeholder(dtype=tf.float32, shape=[self._tSize, None, self._individual_feature_dim], name='individualFeaturePlaceholder')\n",
        "        self._R_placeholder = tf.placeholder(dtype=tf.float32, shape=[self._tSize, None], name='returnPlaceholder')\n",
        "        self._mask_placeholder = tf.placeholder(dtype=tf.bool, shape=[self._tSize, None], name='maskPlaceholder')\n",
        "        self._dropout_placeholder = tf.placeholder_with_default(1.0, shape=[], name='Dropout')\n",
        "        self._nSize = tf.shape(self._R_placeholder)[1]\n",
        "        \n",
        "        \n",
        "        # initialize\n",
        "        # self._state_size = initial_state_size(self.model_params['cell_type_rnn'], self.model_params['num_units_rnn'])\n",
        "        # self._initial_state_placeholder = tf.placeholder(dtype=tf.float32, shape=[1, self._state_size])\n",
        "        # splits = [2*unit for unit in self.model_params['num_units_rnn']]\n",
        "        # self._initial_state = tuple([LSTMStateTuple(*tf.split(value=layer_state, num_or_size_splits=2, axis=1))\n",
        "        #         for layer_state in tf.split(value=self._initial_state_placeholder, num_or_size_splits=splits, axis=1)])\n",
        "        # self._initial_state = self._initial_state[0]\n",
        "\n",
        "        self._rnn_input = tf.expand_dims(self._I_macro_placeholder, axis=0)\n",
        "\n",
        "        with tf.variable_scope(name_or_scope='Model_Layer'):\n",
        "            self.build()\n",
        "        \n",
        "\n",
        "\n",
        "    def build(self):\n",
        "\n",
        "        with tf.variable_scope('RNN_Layer'):\n",
        "            rnn_cell = create_rnn_cell(\n",
        "                        cell_type=self.model_params['cell_type_rnn'],\n",
        "                        num_units=self.model_params['num_units_rnn'],\n",
        "                        num_layers=self.model_params['num_layers_rnn'],\n",
        "                        dp_input_keep_prob=self._dropout_placeholder,\n",
        "                        dp_output_keep_prob=1.0\n",
        "                        )\n",
        "            print(rnn_cell)\n",
        "\n",
        "            rnn_outputs, rnn_state = tf.nn.dynamic_rnn(\n",
        "                        cell=rnn_cell,\n",
        "                        inputs=self._rnn_input,\n",
        "                        # initial_state=self._initial_state,\n",
        "                        dtype=tf.float32)\n",
        "            \n",
        "            self._macro_nn_input = tf.squeeze(rnn_outputs, axis=0) # RNN 통과해서 나온 macro 변수들의 hidden state?\n",
        "\n",
        "        with tf.variable_scope('NN_Layer'):\n",
        "            I_macro_tile = tf.tile(tf.expand_dims(self._macro_nn_input, axis=1), [1,self._nSize,1]) # T * N * macro_feature_dim\n",
        "            I_macro_masked = tf.boolean_mask(I_macro_tile, mask=self._mask_placeholder)\n",
        "            I_masked = tf.boolean_mask(self._I_placeholder, mask=self._mask_placeholder)\n",
        "            I_concat = tf.concat([I_masked, I_macro_masked], axis=1) # None * (macro_feature_dim + individual_feature_dim)\n",
        "            R_masked = tf.boolean_mask(self._R_placeholder, mask=self._mask_placeholder)\n",
        "\n",
        "            h_l = I_concat # 이게 macro와 firm-spec.이 합쳐진 것\n",
        "\n",
        "            with tf.variable_scope('dense_layer_1'):\n",
        "                layer_l = Dense(units=64, activation=tf.nn.relu)\n",
        "                h_l = layer_l(h_l) # macro와 firm-spec.이 합쳐진 것이 Dense layer를 통과함 \n",
        "                h_l = tf.nn.dropout(h_l, self._dropout_placeholder)\n",
        "\n",
        "            \n",
        "            with tf.variable_scope('last_dense_layer'):\n",
        "                layer = Dense(units=1) # 마지막 dense layer이므로 output unit 개수가 1\n",
        "                R_pred = layer(h_l)\n",
        "                self._R_pred = tf.reshape(R_pred, shape=[-1])\n",
        "\n",
        "        self._loss = tf.reduce_mean(tf.square(R_masked - self._R_pred))\n",
        "        self._train_model_op = tf.contrib.layers.optimize_loss(\n",
        "                                                                loss=self._loss,\n",
        "                                                                global_step=self._global_step,\n",
        "                                                                learning_rate=0.01,\n",
        "                                                                optimizer=\"Adam\")\n",
        "\n",
        "\n",
        "    def train(self, sess, dl, dl_valid):\n",
        "        sharpe_train = [] ; loss_train = []\n",
        "        sharpe_valid = []\n",
        "        init_g = tf.global_variables_initializer()\n",
        "        init_l = tf.local_variables_initializer()\n",
        "        sess.run(init_g)\n",
        "        sess.run(init_l)\n",
        "        \n",
        "        for epoch in range(20):\n",
        "            print('Epoch %d' %epoch)\n",
        "            for _, (I_macro, I, R, mask) in enumerate(dl.iterateOneEpoch(subEpoch=False)):\n",
        "\n",
        "                fetches = [self._train_model_op]\n",
        "                feed_dict = {self._I_macro_placeholder:I_macro,\n",
        "                            self._I_placeholder:I,\n",
        "                            self._R_placeholder:R,\n",
        "                            self._mask_placeholder:mask,\n",
        "                            self._dropout_placeholder:self.model_params['dropout']\n",
        "                            }\n",
        "                \n",
        "                sess.run(fetches=fetches, feed_dict=feed_dict)\n",
        "            \n",
        "            ### evaluate train loss / sharpe\n",
        "            train_epoch_loss = self.evaluate_loss(sess, dl)\n",
        "            train_epoch_r2 = self.evaluate_r_sq(sess, dl)\n",
        "            print('train loss: ', train_epoch_loss)\n",
        "            print('train R-Squared: ', train_epoch_r2)\n",
        "            loss_train.append(train_epoch_loss)\n",
        "\n",
        "    def evaluate_loss(self, sess, dl):\n",
        "        for _, (I_macro, I, R, mask) in enumerate(dl.iterateOneEpoch(subEpoch=False)):\n",
        "            feed_dict = {self._I_macro_placeholder:I_macro,\n",
        "                        self._I_placeholder:I,\n",
        "                        self._R_placeholder:R,\n",
        "                        self._mask_placeholder:mask,\n",
        "                        self._dropout_placeholder:1.0\n",
        "                            }\n",
        "            loss, = sess.run([self._loss], feed_dict=feed_dict)\n",
        "        return loss\n",
        "\n",
        "    def getPrediction(self, sess, dl):\n",
        "        for _, (I_macro, I, R, mask) in enumerate(dl.iterateOneEpoch(subEpoch=False)):\n",
        "            feed_dict = {self._I_macro_placeholder:I_macro,\n",
        "                        self._I_placeholder:I,\n",
        "                        self._R_placeholder:R,\n",
        "                        self._mask_placeholder:mask,\n",
        "                        self._dropout_placeholder:1.0}\n",
        "            R_pred, = sess.run(fetches=[self._R_pred], feed_dict=feed_dict)\n",
        "        return R_pred\n",
        "\n",
        "    def evaluate_r_sq(self, sess, dl):\n",
        "        R_pred = self.getPrediction(sess, dl) ; print(R_pred.shape)\n",
        "        for _, (I_macro, I, R, mask) in enumerate(dl.iterateOneEpoch(subEpoch=False)):\n",
        "            r_sq = 1 - ( sum((R[mask] - R_pred)**2) / sum(R[mask]**2) )\n",
        "        return r_sq"
      ],
      "metadata": {
        "id": "NTNpZClyiEp8"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "global_step = tf.train.get_or_create_global_step()\n",
        "model = Model(model_params=config, tSize=240, global_step=global_step)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrljwJnSALU0",
        "outputId": "df2bd36c-fd2f-4997-ae37-d3daa370f1f7"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tensorflow.python.ops.rnn_cell_impl.DropoutWrapper object at 0x7fe1843a3390>\n",
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fe1843a33d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fe1843a33d0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fe1843a33d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fe1843a33d0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe184e5bfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe184e5bfd0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe184e5bfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe184e5bfd0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe17be52790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe17be52790>>: AttributeError: module 'gast' has no attribute 'Index'\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe17be52790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe17be52790>>: AttributeError: module 'gast' has no attribute 'Index'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "model.train(sess, dl, dl_valid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRk18vpSBTEx",
        "outputId": "449378d7-5adc-4cb0-f895-d6237d91248d"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n",
            "(336113,)\n",
            "train loss:  0.030410819\n",
            "train R-Squared:  -0.8283528237461184\n",
            "Epoch 1\n",
            "(336113,)\n",
            "train loss:  0.02918724\n",
            "train R-Squared:  -0.754789152138359\n",
            "Epoch 2\n",
            "(336113,)\n",
            "train loss:  0.02674354\n",
            "train R-Squared:  -0.6078695944862302\n",
            "Epoch 3\n",
            "(336113,)\n",
            "train loss:  0.024638131\n",
            "train R-Squared:  -0.481288522257006\n",
            "Epoch 4\n",
            "(336113,)\n",
            "train loss:  0.021807054\n",
            "train R-Squared:  -0.31107920566575387\n",
            "Epoch 5\n",
            "(336113,)\n",
            "train loss:  0.0201662\n",
            "train R-Squared:  -0.21242807322760138\n",
            "Epoch 6\n",
            "(336113,)\n",
            "train loss:  0.01953604\n",
            "train R-Squared:  -0.1745417696542093\n",
            "Epoch 7\n",
            "(336113,)\n",
            "train loss:  0.019178407\n",
            "train R-Squared:  -0.1530402077057138\n",
            "Epoch 8\n",
            "(336113,)\n",
            "train loss:  0.018774021\n",
            "train R-Squared:  -0.1287277658872461\n",
            "Epoch 9\n",
            "(336113,)\n",
            "train loss:  0.01820862\n",
            "train R-Squared:  -0.0947347489843513\n",
            "Epoch 10\n",
            "(336113,)\n",
            "train loss:  0.017426161\n",
            "train R-Squared:  -0.04769199653145151\n",
            "Epoch 11\n",
            "(336113,)\n",
            "train loss:  0.016581897\n",
            "train R-Squared:  0.0030667276273956823\n",
            "Epoch 12\n",
            "(336113,)\n",
            "train loss:  0.015906543\n",
            "train R-Squared:  0.04367022525476871\n",
            "Epoch 13\n",
            "(336113,)\n",
            "train loss:  0.015531578\n",
            "train R-Squared:  0.06621374468697572\n",
            "Epoch 14\n",
            "(336113,)\n",
            "train loss:  0.015391369\n",
            "train R-Squared:  0.07464332824225361\n",
            "Epoch 15\n",
            "(336113,)\n",
            "train loss:  0.015285239\n",
            "train R-Squared:  0.08102405914102828\n",
            "Epoch 16\n",
            "(336113,)\n",
            "train loss:  0.015133045\n",
            "train R-Squared:  0.09017424230701576\n",
            "Epoch 17\n",
            "(336113,)\n",
            "train loss:  0.014935491\n",
            "train R-Squared:  0.10205154961968976\n",
            "Epoch 18\n",
            "(336113,)\n",
            "train loss:  0.014652525\n",
            "train R-Squared:  0.11906390510816778\n",
            "Epoch 19\n",
            "(336113,)\n",
            "train loss:  0.014335165\n",
            "train R-Squared:  0.1381442354301552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gA8crPTfoXGL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}